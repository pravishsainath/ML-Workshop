{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AIC_Logistic_Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "F1woU2CU_Jz4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# <div align=\"center\">  Artificial Intelligence Concordia Workshop <br /> </div>\n",
        "## <div align=\"center\"> Logistic Regression</div>\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"https://github.com/olibel270/Workshop1JupyterNote/blob/master/images/AICLogo.jpg?raw=1\" style=\"width: 300px\" /></div>\n",
        "\n",
        "---------------------------------------------------------------------\n",
        "\n",
        "<div align=\"right\">\n",
        "  Follow us on:<br />\n",
        "[ Facebook](https://www.facebook.com/AISConU/)<br />\n",
        "[Our Website](https://www.aisconcordia.com)<br />\n",
        "  </div>\n",
        " \n"
      ]
    },
    {
      "metadata": {
        "id": "QM9C4PbO_Li2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "metadata": {
        "id": "IUN155XV_Lv1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Logistic Regression is generally used for **classification** and not regression. \n",
        "\n",
        "Unlike Linear Regression, the output can take a limited number of values only i.e, the **output is categorical** (discrete).\n",
        "\n",
        "When the number of possible outcomes is only two it is called **Binary Logistic Regression**."
      ]
    },
    {
      "metadata": {
        "id": "WB20PBYS_L-V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Linear Regression just takes the weighted sum of inputs to get the outputs. There is no restriction on the value it computes.\n",
        "For a binary classification, we want a value between 0 and 1 to map it as a probability and determine the output class. \n",
        "That’s why Linear Regression can’t be used for classification tasks."
      ]
    },
    {
      "metadata": {
        "id": "75jMibOP_MOJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "An activation function called the sigmoid function is used for this transformation to a probability.\n",
        "\n",
        "The sigma function $ \\sigma(z)  $ is given by :\n",
        "\n",
        "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} $$\n",
        "\n",
        "\n",
        "The plot of the sigmoid function looks like this :\n",
        "\n",
        "![sigmoid function](https://cdn-images-1.medium.com/max/1600/1*yKvimZ3MCAX-rwMX2n87nw.png)"
      ]
    },
    {
      "metadata": {
        "id": "yJMqk5nO_MbE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* The value of the sigmoid function always lies between 0 and 1. \n",
        "* The value is exactly 0.5 at z = 0. \n",
        "\n",
        "Therefore, we can use 0.5 as the probability threshold to determine the classes. \n",
        "\n",
        "If the probability is greater than 0.5, we classify it as Class-1 (y=1) or else as Class-0 (y=0)."
      ]
    },
    {
      "metadata": {
        "id": "tQbnkPjoESh-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "ZEf7c29nEENj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* The data consists of marks of two exams for 100 applicants. \n",
        "* The target value takes on binary values 1 or 0. \n",
        "* 1 means the applicant was admitted to the university \n",
        "   0 means the applicant was rejected. \n",
        "   \n",
        "   The objective is to build a classifier that can predict whether an application will be admitted to the university or not."
      ]
    },
    {
      "metadata": {
        "id": "ucr_cK3g_I9U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_tnc\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "igo8ZKu7Izy5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset"
      ]
    },
    {
      "metadata": {
        "id": "N5U-1PoJEna5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Function to load csv data into Pandas data frame\n",
        "\n",
        "def load_data(path, header):\n",
        "    marks_df = pd.read_csv(path, header=header)\n",
        "    return marks_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "se6PYYPgEyNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data():\n",
        "  # load the data from the file\n",
        "  data = load_data(\"marks.csv\", None)\n",
        "\n",
        "  # X = feature values, all the columns except the last column\n",
        "  x = data.iloc[:, :-1]\n",
        "\n",
        "  # y = target values, last column of the data frame\n",
        "  y = data.iloc[:, -1]\n",
        "  \n",
        "  x_train, y_train = x[:70], y[:70]\n",
        "  x_test, y_test = x[-30:], y[-30:]\n",
        "  \n",
        "  return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WIDestqVRQYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = get_data()\n",
        "\n",
        "print(\"x_train shape = \",x_train.shape)\n",
        "print(\"y_train shape = \",y_train.shape)\n",
        "print(\"x_test shape = \",x_test.shape)\n",
        "print(\"y_test shape = \",y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3vvP6bSaIwAV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualizing the dataset"
      ]
    },
    {
      "metadata": {
        "id": "lVCQjPEzGDqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def visualize_data(x,y):\n",
        "  # filter out the applicants that got admitted\n",
        "  admitted = x[y == 1]\n",
        "\n",
        "  # filter out the applicants that din't get admission\n",
        "  not_admitted = x[y == 0]\n",
        "\n",
        "  # plots\n",
        "  plt.scatter(admitted.iloc[:, 0], admitted.iloc[:, 1], label='Admitted',color='g')\n",
        "  plt.scatter(not_admitted.iloc[:, 0], not_admitted.iloc[:, 1], label='Not Admitted',color='r')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Marks in Subject 1')\n",
        "  plt.ylabel('Marks in Subject 2')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ni5MFiBYR8Ey",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_data(x_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3unRENFXJICD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Hypothesis function"
      ]
    },
    {
      "metadata": {
        "id": "-CzDyLLcJILg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For logistic regression, the assumed hypthesis function is :\n",
        "\n",
        "$$ h(x) = \\sigma(\\theta^T x)$$\n",
        "\n",
        "\n",
        "\n",
        "If the weighted sum of inputs is greater than zero, the predicted class is 1 and vice-versa. So the decision boundary separating both the classes can be found by setting the weighted sum of inputs to 0.\n",
        "\n",
        "\n",
        "\n",
        "Using the sigma function $ \\sigma(\\theta^T)  $ is given by :\n",
        "\n",
        "$$ h(x) = \\frac{1}{1+e^{-\\theta^Tx}} $$\n"
      ]
    },
    {
      "metadata": {
        "id": "kK1rH2ERIljl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def transform_data(x,y):\n",
        "  # Adding a column of ones for the bias term\n",
        "  X = np.c_[np.ones((x.shape[0], 1)), x]\n",
        "\n",
        "  # Converting vector y to a   n x 1  matrix\n",
        "  Y = y.reshape(y.shape[0],1)\n",
        "  \n",
        "  return X,Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FDFagpmKuZK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(X):\n",
        "    # Activation function used to map any real value between 0 and 1\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "def linear_output(theta, X):\n",
        "    # Computes the weighted sum of inputs\n",
        "    return np.dot(X, theta)\n",
        "\n",
        "def probability(theta, X):\n",
        "    # Returns the probability after passing through sigmoid\n",
        "    return sigmoid(linear_output(theta, X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pIY0QpWX4BOB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*qKfAYUsI0VPcIXVBbEdPEg.png)\n",
        "\n",
        "  </center>"
      ]
    },
    {
      "metadata": {
        "id": "VmB_dS1L4ime",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define **cross-entropy loss** :\n",
        "\n",
        "$$  cost(h(x),y) = - y log((h(x))) - (1 - y) log(1 - h(x)) $$"
      ]
    },
    {
      "metadata": {
        "id": "kFOq02_HK_hb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cost_function(theta, X, Y):\n",
        "    # Computes the cost function : cross-entropy loss for all the training samples\n",
        "    n = X.shape[0]\n",
        "    total_cost = -(1 / n) * np.sum(Y * np.log(probability(theta, X)) + (1 - Y) * np.log(1 - probability(theta, X)))\n",
        "    return total_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QAOZruYVLG1-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient(theta, X, Y):\n",
        "    # Computes the gradient of the cost function at the point theta\n",
        "    n = X.shape[0]\n",
        "    return (1 / n) * np.dot(X.T, sigmoid(linear_output(theta,   X)) - Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1r-uR7dtLIdo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fit(X, Y):\n",
        "    # Initialization for the parameter vector theta = [theta_0  theta_1 theta_2]\n",
        "    theta = np.zeros((X.shape[1],1))\n",
        "    optimal_weights = fmin_tnc(func=cost_function, x0=theta, fprime=gradient, args=(X, Y.flatten()))\n",
        "    return optimal_weights[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4VOKysCmQeKt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "5ECEic4fL5-v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train,Y_train = transform_data(x_train,y_train)\n",
        "parameters = fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fpkbfl7tPlI_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Print the learned parameters \n",
        "parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tjkP4HFvPmVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_values = np.arange(20,100,10)\n",
        "y_values = - (parameters[0] + np.dot(parameters[1], x_values)) / parameters[2]\n",
        "\n",
        "plt.plot(x_values, y_values, label='Decision Boundary')\n",
        "plt.scatter(admitted.iloc[:, 0], admitted.iloc[:, 1], label='Admitted',color='g')\n",
        "plt.scatter(not_admitted.iloc[:, 0], not_admitted.iloc[:, 1], label='Not Admitted',color='r')\n",
        "plt.xlabel('Marks in 1st Exam')\n",
        "plt.ylabel('Marks in 2nd Exam')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I900ozGXQjq3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X, theta):\n",
        "    return probability(theta, X)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QQ1MhmtRGoR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def accuracy(X, actual_classes, theta, probab_threshold=0.5):\n",
        "    predicted_classes = (predict(X, theta) >= probab_threshold).astype(int)\n",
        "#     predicted_classes = predicted_classes.flatten()\n",
        "    accuracy = np.mean(predicted_classes == actual_classes)\n",
        "    return accuracy * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rMRFyPv9vQgJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training accuracy\n",
        "accuracy(X_train, Y_train.flatten(), parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4Qw1u6jTtq4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test accuracy\n",
        "X_test,Y_test = transform_data(x_test,y_test)\n",
        "accuracy(X_test, Y_test.flatten(), parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hOS13pPnsIMA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## SK Learn Implementation"
      ]
    },
    {
      "metadata": {
        "id": "bfDqWpCQTx0g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score \n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)\n",
        "predicted_classes = model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test,predicted_classes)\n",
        "sklearn_parameters = model.coef_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T8gF4__QVopg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"SkLearn Parameters = \",sklearn_parameters)\n",
        "print(\"SkLearn Test accuracy = \",accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}